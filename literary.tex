%    This program is free software: you can redistribute it and/or modify
%    it under the terms of the GNU General Public License as published by
%    the Free Software Foundation, either version 3 of the License, or
%    (at your option) any later version.
%
%    This program is distributed in the hope that it will be useful,
%    but WITHOUT ANY WARRANTY; without even the implied warranty of
%    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%    GNU General Public License for more details.
%
%    You should have received a copy of the GNU General Public License
%    along with this program.  If not, see <http://www.gnu.org/licenses/>.\documentclass[a4paper,11pt]{article}
%
%    Copyright: John A Stevenson, University of Edinburgh, Twitter: @volcan01010

\documentclass[a4paper,12pt,twocolumn]{article}

%%%%%%%%%%% PAGE SIZE: Set up page with 2 cm margins
\usepackage{anysize}
\marginsize{2cm}{2cm}{2cm}{0cm}

%%%%%%%%%%% FONTS:
\usepackage[english]{babel} 
\usepackage[T1]{fontenc} % Font encoding for Icelandic characters
\usepackage{textgreek} % Greek letters without going into Math mode
\usepackage{eulervm} % This makes mathtext upright, not italic

\usepackage[scaled]{helvet} % Helvetica.  Similar to Arial as specified by NERC
\renewcommand*{\familydefault}{\sfdefault} % Helvetica needs default font to be SansSerif
%\usepackage{libertine} % Linux Libertine font is a nice serif font
%\usepackage{lmodern} % use Latin Modern for that LaTeX look

%%%%%%%%%%% REFERENCES: 
%\usepackage[round]{natbib} % Bibliography / reference package
\usepackage{multicol} % Make reference list multicolumn

%%%%%%%%%%% FIGURES:
\usepackage{graphicx} % Includes graphics
\usepackage{float} % Allows forcing of figure locations
\usepackage{wrapfig} % Wraps text round images
% Change figure caption font
\usepackage[font=small,format=plain,labelfont=bf,up,textfont=it,up]{caption}

%%%%%%%%%%% SPACE SAVERS:
\usepackage{mdwlist} % less gaps in itemize sections
\usepackage{titlesec} % format of section titles
\titlespacing{\section}{0pt}{8pt}{4pt}
\titlespacing{\subsection}{0pt}{6pt}{2pt}
%\usepackage{setspace}
%\onehalfspacing
\linespread{0.96}

\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}
\setlength{\footskip}{0.6cm}
%\setlength{\bibsep}{0pt}
\setlength{\parskip}{0.5\parskip}
\setlength{\textfloatsep}{0.5\textfloatsep}

%%%%%%%%%%% OTHER:
\usepackage{hyperref} % Hyperlinks in pdf (remove 'hidelinks' to display)
\usepackage[dvipscolors]{xcolor}

% Gantt chart package
\usepackage{pgfgantt}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
\usepackage[modulo]{lineno}

\title{Towards literary engineering: algorithm and tools for
  evaluating, enhancing and outright producing literary works}
\author{J. J. Merelo}
\date{\today}

\begin{document}
\twocolumn[
  \begin{@twocolumnfalse}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

\smallskip
\hrule height 1pt 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Outline}
\begin{quote}
  This project proposes the construction of a methodology and
  set of tools that aim to assess the quality, identify authors and,
  eventually, use those measures for the improvement of written texts
  (including hypertext), bringing the era of computer-aided
  copy-editing and literature in the same way computer-aided drawing
  has helped, and continues to help, art as well as engineering.
\end{quote}
\rule{\textwidth}{1pt}
\end{@twocolumnfalse}
]
\section{Introduction}

The editorial industry is in turmoil since the extensive consumption
of ebooks and the creation of self-publishing platforms such as Kindle
Direct Publishing, iTunes and Lulu. These platforms imply that many more actors are accessing the market, but at the same time, due to low profit levels, it is complicated or even impossible to live professionally off it, since hundreds of daily sales are needed for even a minimum wage.

At the same time, the quality of these works is variable. The
publishing industry has many tools: correctors, editors, career
managers and translators that are not available to single, independent
authors, or are at a high cost. These {\em literary tools} are mainly manual and have not been
automated beyond grammar and spelling corrections. The product of translation tools,
at the same time, has eventually to be reviewed by a human to reach a
good level of quality.

From the point of view of textual authors, they are at the same level
graphic designers where 20 years ago, animation artists were 80 years ago and comic book writers were 50
years ago. Or architects were a hundred years ago. Most art (or, in
contemporary terms, content creation) uses tools to automate part of
the work. Architects (physical and software) reuse patterns to build;
comic writers, for a long time, have used patterns and textures that
can be transferred to the page and they, along with graphic
illustrators, have used freehand software tools to perform all kind of
tasks, from simple fills to more complex transitions. Animation artist
work on models, and, instead of painstakingly moving physical models
of their characterss  Harryhausen-style create models which are then
used in complex software pipelines to create motion and, eventually,
the whole motion picture.

However, writers, barring plagiarism, do have to sit down to write,
letter by letter and sentence by sentence, their whole work, and this
affects creative as well as commercial writers. The most advanced tool
that they use is a typo and grammar checker, but even so the draft has
to go through several human eyes and minds to be corrected, polished
at several levels (is this character necessary? Is this final coherent
and genre-caononical?)

This project is the first step in the creation of methodologies, algorithms and
software tools that aid in automation of the whole literary creation and
copyediting process: from idea to final typesetting, and even improvement in new editions
based on the automatic processing of literary reviews or any other
kind of feedback received by the text.

This project will match natural language processing tools for
analyzing original works as well as reviews, ontologies and thesaurus
to improve the quality of written text based on several metrics: similarity to
text-mined classic works and diversity and {\em style} parameters
assigned automatically to the work, and eventually improvement of the
manuscript using metaheuristics such as evolutionary algorithms or
simulated annealing. In the case of published text, {\em quality}
assessment can include metrics that depend on the perceived fitness of
the text, including reviews and, in the case of description of
products, sales.

The project involves the creation of new algorithms for text
improvement, sentiment analysis to gauge the reaction to a particular
work mining social networks and review sites, and automatic quality
evaluation based on text mining, as well as development of open-source
literary engineering tools that can be used by authors as a help to
creation as well as editorials as decision-support systems.  

Several scenarios are foreseen: an author using the tool to improve a
literary manuscript as well as creating new editions based on social
network reviews; a games creator using the tool to improve backstories
for characters; interactive fiction apps or websites that change story
based on user interaction and reaction and automatic adaptation of
text description of products in an online website depending on the
customer profile or the interaction of customers with it. 

The rest of the proposal is organized as follows. Next we present the
state of the art in {\em literary engineering}. Following up we will
present the long-term objectives of research, short-term objectives
for this proposal, and minimal objectives that will be achieved at the
end of the funded period. We will also present our experience in this
area, and finally our data policies and budget requests.

\section{State of the art}

In general, computational linguistics  has received a lot of
attention lately, with yearly symposiums devoted to it and many
authors advocating computational approaches to text analysis, as
opposed to purely qualitative ones \cite{roque2012towards}. The main lines
of research are concentrated around analysis and comparison of texts;
analysis tools include complex networks
\cite{1367-2630-14-4-043029,0295-5075-100-5-58002} whose extraction
from literary fiction is not trivial \cite{elson2010extracting}, but,
once done, allows a whole new set of tools for analyzing the structure
of literary works \cite{seo14:snf};  writing-style
features \cite{ASI:ASI20316} are also used for this kind of analysis
and phrase structure fragments \cite{van2012literary}, which has
recently been used to compare different kinds of literary corpus
\cite{jautze2013high}. These two papers have been the result of the
project \href{http://literaryquality.huygens.knaw.nl/}{The riddle of
  literary quality}, a institute by the Huygens institute for the
History of the Netherlands, funded by the Computational Humanities
Programme. It is indeed, interesting to see this kind of initiatives,
since they prove that measuring literaty quality computationally is
still a challenge; in fact, we intend to take this problem a bit
further by aiming to {\em improve} the quality of a literaty text
using quality measures and stochastic optimization algorithms. 

In fact, there are some works in this direction already; proactive evolution of text for its optimization has been done so far
in a very controlled environment, such as technical texts
\cite{Rascu06acontrolled,hernandez2004checking}. Doing it in a less
constrained envirohment or incorporating interactive features
has not been done so far, although nowadays is an easily available
metric, since it is very easy to measure the success of an Instagram
picture or Facebook post (the last of which is more {\em literary} in
the traditional sense, although the extensive use of hashtags by the
former could be also open to optimization). We have also experimented
by generating stories \cite{2014arXiv1403.3084G} and even whole books
\cite{Merelo201401}, but no attempt has been made to make it even
remotely literary. Content-generation tools could be combined with
content-improvement (which we call {\em literary engineering}) tools
to assist in the creation of high-quality literary (and commercial)
works, or simply increase the productivity and leave the purely
creative work to the computing engine that can do it better; the human
brain. 

Even more so, criticism can also be gauged to measure the
quality of the text it is talking about. Finally, personalization of
text, so far, has not been done to the best of our knowledge.

That is why this project will advance the state of the art in several
areas, be them algorithmic, methodological or purely technical,
providing open source tools to carry out text-improvement tasks. 

\section{Objectives}
\label{sec:obj}

The main objective is to create a tool that is able to improve the literary
quality of a written text. This objective is divided in several
sub-objectives. \begin{itemize}
\item Examine different measures of literary quality and create an
  aggregated value (or {\em fitness} that describes in a single value
  (or multiple criteria) the quality of a work.
\item Optimize text-analysis tools to be able to make a fast model of
  a written work.
\item Create an abstract phrase model that will be used to represent
  it and, eventually, optimize it
\item Design an evolutionary algorithm for optmization of literary
  texts that uses as fitness the measure described in the first bullet
  point, as representation the model described in the third bullet
  point which has been obtained in a reasonable amount of time thanks
  to the tool created in the second bullet point
\end{itemize}

A non-scientific objective will be to follow an open science
methodology during the whole duration of the project. Our team is
committed to open science, and offers as a proof former work \href{http://github.com/JJ}{developed
and described in GitHub} and the already finished project
\href{http://canube.wordpress.com}{CANUBE}, which was locally
funded. We will also draw from our own experience writing an open
source novel, \href{http://jj.github.io}{hosted at GitHub} which
allowed us to have more information than is usually available after a
novel is written and published, information that could, indeed, be
used to help with the writing process and improve the resulting text. 

This will be our minimum achievable objective\begin{enumerate}
\item Choose ten public-domain, creative commons or automatically
  generated short stories (for instance, backstories generated by our
  system MADE \cite{2014arXiv1403.3084G}.
\item Design and implement, using available open source tools (such as our Perl
  evolutionary algorithm module {\tt Algorithm::Evolutionary}
  \cite{ae09} or our node.js EA module \cite{nodeo2014}) a
  text-improvement evolutionary algorithm. 
\item Obtain a 10\% improvement of the Fog and Flesch index of said
  texts with a runtime of less than 24 hours. 
\end{enumerate}

In the next section we will explain the steps we will be taking,
should this proposal be funded, to
achieve this minimum objective and go beyond it. 

\section{Methodology}
\label{sec:meth}

In this section we will have a look on how the different objectives of
this proposal are going to be tackled. Next we will talk about
literary metrics or, indeed, the {\em riddle} of literary quality, a
riddle we will have to sove, at least partially, if we want to {\em
  improve} it automatically

\subsection{Literary metrics}

The relationship of Mathematics, and thus computer science with
literature was probably found by surrealists, but the first author
that was attracted to mathematics as a source of inspiration was
Raymond Queneau, a French author and translator who also became a
member of the French Mathematics Society, writing a book entitled {\em
  The fundaments of literature after David Hilberg},
\cite{queneau1976fondements}. He was mentioned as wanting to introduce
mathematic, that is, algorithmic, notions into poetry or the creation
of a novel as soon as the twenties, more or less contemporarily to
surrealism. \cite{emmer2005mathematics}. In fact, combinatorics played
a big role in his work ``Exercises in Style''
\cite{queneau2013exercises}, with 99 variations of a single story
written, by hand each time, in different styles. Mathematics is, in
this sense, more an inspiration than an actual methodology. 

And a mathematical methodology for generating or improving literature
calls for literary analytics. Measures such as Gunning ``fog'' index
\cite{gunning1969fog} or Flesch index \cite{roberts1994effects}, which
measure readability (not quality), but are useful analytics when you
want to address a text to a certain audience (for instance, patients'
families \cite{grossman1994informed}). 

In fact literary quality is a riddle since it involves syntactic and
semantic measures, as well as purely emotional ones to create an aggregated value. This
  measure will integrate measures obtained by text analysis together
  with user-generated measures (scores obtained via Amazon or Google
  Book Reviews or goodreads, {\em likes} or {\em stars} in social
  networks, and text-mining of published reviews in literary magazines
  or answers. We will include in this measure similarity measures
  obtained by analysis of classical (in general, public domain)
  works. 

So the first step in our methodology will be to delve into the puzzle
of literary quality and choose a measure, or a set of measures, that
should be optimized in order to make a text a {\em better} text. In
the process we will find whether literary quality is, in fact, a
single-objective problem, with many measures correlated with each
other, or a multimodal algorith, with multiple measures independent
with each other and needing a multiobjective optimization algorithm to
deal with them. In fact, {\em readability} measures with which we have
had certain experience are correlated, but as soon as we introduce
other measures this might not be true. At the end of this period, we
will have a set of automatic quality measures taht we will apply to
texts to perform a multi- or single-objective optimization process on
them. 

These tools will have to be applied to the text, probably preceded by
a preprocessing phase where it will be analyzed. We will talk about
this phase in the next subsection.

\subsection{Creating an abstract sentence representation}

The main objective of this phase in the investigation will be to find,
from a particular sentence, which possible alternatives are
possible. That calls for identifying {\em important} (and also {\em malleable} words in the
phrase (subjects, adjectives, verbs) which could be subject to
change. 

A word by word change is not the only, or even the best, way of
improving a text. But it has been experimented in commercial
copy-editing for a long time, using only certain words and a limited
amount of possible substitutions. For instance, a sentence like this
one
\begin{quote}
This widget is the best in its category
\end{quote}
could be reformulated with possible alternatives
\begin{quote}
This \{widget, apparatus, engine, machine, device\} is the \{best, most
  robust, most powerful, best known\} in its \{class, category, kind\}
\end{quote}
, and obtaining the best combination according to some index (number
of words or the aforementioned Fog or Flesch indices) would be a
matter of using any combinatorial optimization algorithm.

However, even if the optimization phase is done automatically, writing
the phrase, selecting possible words for improvement and creating
possible alternatives is done by hand, with resulting low
productivity. 

What we propose in this project is to do that phase automatically and
for large (several hundreds of sentences) texts. That processing is
done by so-called \href{http://en.wikipedia.org/wiki/Part-of-speech_tagging}{{\em part-of-speech} taggers}, which involves a
process of word-category disambiguation. A {\em tagged} sentence
includes a model of the sentence with each word labelled (tagged) with
its position in the sentence, and at the same time a sentence
structure (that could, itself, be an object of optimization, although
this is not our objective in this project).

This phase is, initially, an evaluation phase, since there are many
open source available tools that are able to perform it on
unconstrained English text. However, that \href{http://mattwilkens.com/2008/11/08/evaluating-pos-taggers-speed/}{the time they take to analyze a single sentence} can be quite
high. We will part from off-the-shelf open source implementations like
\href{http://gate.ac.uk/wiki/twitter-postagger.html}{GATE} and, if needed, will
apply best-of-breed techniques to optimize for speed so that whole
1-to-10K word corpuses can be analyzed in a reasonable amount of
time. Current POST seem to run at speeds from 13 words per second to
5K; in any case, speed seems to be an issue and we want to take the
contingency of low performance of POST into account in this project in
case it arises. 

Once parts of speech have been tagged, a second processing phase will
look for possible alternatives. Some words like ``word'' will have no
alternative. But some like {\em tree} might be substituted by hyponyms
like {\em oak tree} or {\em poplar} or hypernyms like {\em plant}.
Some words will be quite malleable ({\em rain}) while others might be
much less {\em fiduciary}. At any rate, by using ontologies like
Wordnet (or equivalent in other languages) we will build a hyper-model
of the text by substituting every word by a list of possible
substitutions.

The text will then be represented by one of the different
alternatives; a set of values for these alternatives will form the
{\em chromosome} for the evolutionary algorithm that will be run on
them \cite{eiben2002evolutionary}. This is a combinatorial
optimization problem (finding the best combination) for which
ready-made solutions already exist. However, its sheer size will
present some challenges that will also have to be solved. This will be
addressed in the next section, that deals with the evolutionary
algorithm itself.

\subsection{Evolutionary algorithm for combinatorial/literary optimization}

The main thrust of this project lies in the first two objectives, and
specially in the development of a fast and reliable literary
metric. However, we are dealing with a search problem in a potentially
huge space which will need specific techniques to deal with it.

Several methods can be used for combinatorial optimization. In
general, simulated quenching
\cite{DBLP:journals/apin/CastilloARMGLG12} obtains results that are
faster, but less accurate, than evoltutionary algorithms, and has been
applied successfully to another combinatorial optimization problem,
the MasterMind puzzle \cite{jj-ppsn96}. This algorith, available in
our {\tt Algorithm::Evolutionary} library \cite{ae09}, can be readily
tried as a first approximation. However, evolutionary algorithms are
slower but more efficient, so we will test different implementations,
including, eventually, distributed computing or even volunteer
browser-based implementations \cite{gecco07:workshop:dcor,} to obtain
a solution in a reasonable amount of time. 

The evolutionary algorithm will have to adapt to the single or
multi-objective metrics chosen in the first step. Any of them has been
well studied and there are many implementations available, from ourn
own to the libraries written in Java \href{DBLP:conf/iwann/MereloRACML11}{ECJ} (which is the closest thing
to a standard, with several dozens versions so far) or \href{http://code.google.com/p/deap/}{DEAP} (in
Python, with increasing popularity). 

Implementation is always important
\cite{DBLP:conf/iwann/MereloRACML11} and first results obtained with
it might lead to a reformulation
of the evolutionary algorithm used, opting for more exotic forms of
implementation (map/reduce implementations or dataflow computing). 

At any rate, this project is more application-oriented than
basic-science. Even if we attempt to start a new and fruitful line of
investigation with it, one year is one year and we cannot solve all
possible problems, so in many cases we will opt (maybe fine-tuned)
off-the-shelf technology that is readily available when possible.

This and other horizontal issues will be the matter of the next
subsection,

\subsection{Horizontal issues and temporization}

Since it is a project that will take a single year, we will have only
two workpackages.

\subsubsection{Workpackage one}

This package will include all research and development work. It will
be carried by the Principal Investigator as well as the hired PhD
student.

\begin{itemize}
\item {\bf Task 1.1} Addresses first objetive: analysis and formulation of
  quality metrics for literary texts. Release of software implementation.
 {\bf Deliverable}: technical
  report and/or conference paper. Duration: months 0 to 3.
\item {\bf Task 1.2} Creation of models of literary texts by merging
  part-of-speech tagging and ontolologies. {\bf Deliverable}: technical
  report and/or conference paper.  Release of a new version of
  software including this. Duration: months 4 to 7. 
\item {\bf Task 1.3} Design and implementation of an algorithm for
  optimization of literary texts. {\bf Deliverable}: technical
  report and/or conference paper. Final release (for the project) of
  the open source implementation.  Duration: months 7 to 12.
\end{itemize}

The PI and our research group GeNeura is committed to open science, so
all software will be released under a strong open source license
whenever upstream libraries allow it.

\subsubsection{Workpackage two}

This package will include ancillary work, including administration,
communication and community engagement. Each will receive a different
task, with rolling deliverables and a duration that will span the
whole project, 12 months.


 



\section{Our experience}
\label{sec:exp}

GeNeura team (http://geneura.wordpress.com) includes a published
writer that has won literary prices in its cast, which has them
interested in this kind of things. They have been working in the
advertising industry some time in the past
\cite{merelo:ecal97,AISB97}, but also have experience in interactive
art \cite{DBLP:conf/cec/TrujilloVVG13,DBLP:conf/cec/FernandesIBRG11}.


We also have experience in performance evaluation and prediction
\cite{castillo:evostar08,hardwareevo}, as well as in the optimization of
software-defined architectures \cite{gecco08:castillo}.

Our complex systems experience arises from our early interest in
artificial life \cite{ecal93} but have lately applied complex network
analysis to co-authorship in particular areas
\cite{ec-network-2007,merelo2013complex,DBLP:journals/corr/abs-1108-0261}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Acknowledgements}
\textit{This \LaTeX template was produced by John A Stevenson, and originally published \href{http://all-geo.org/volcan01010/2013/07/grant-applications-are-hard-work-includes-latex-template}{here}.  For volcanology and scientific computing news, visit the blog at \href{http://all-geo.org/volcan01010}{http://all-geo.org/volcan01010} or follow him on Twitter \href{https://twitter.com/volcan01010}{@volcan01010}.}

\bibliographystyle{alpha}
\twocolumn[
  \begin{@twocolumnfalse}
\bibliography{literary,geneura}
\end{@twocolumnfalse}
]

\section{Data Policy}

\section{Budget}


\end{document}
